"""ä¸šåŠ¡æ•°æ®ä¸Šä¼ é¡µé¢ï¼ˆå•æ–‡ä»¶ï¼‰ã€‚"""

from __future__ import annotations

import os
import sys
from pathlib import Path
from typing import Dict, List, Optional

import streamlit as st
from dotenv import load_dotenv
from sqlalchemy import create_engine
from sqlalchemy.engine import URL

BASE_DIR = Path(__file__).resolve().parents[2]
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

from db.services.db_config import get_business_db_config
from db.services.init_system_db import init_system_db
from db.services.metadata import (
    sync_business_metadata,
    update_business_column_display_names,
)
from db.services.upload import FileFormatError, upload_csv, upload_excel
from analysis.settings.config import ensure_database_exists_for_config

load_dotenv(BASE_DIR / ".env")

def _parse_list_env(name: str, default: List[str]) -> List[str]:
    raw = os.getenv(name)
    if raw is None:
        return list(default)
    items = [part.strip() for part in raw.split(",") if part.strip()]
    return items


def _parse_display_row_env(name: str) -> tuple[Optional[int], str]:
    raw = os.getenv(name)
    if raw is None:
        return None, "è‡ªåŠ¨è¯†åˆ«"
    text = raw.strip()
    if not text:
        return None, "è‡ªåŠ¨è¯†åˆ«"
    try:
        value = int(text)
    except ValueError:
        return None, f"è‡ªåŠ¨è¯†åˆ«ï¼ˆæ— æ•ˆé…ç½®: {text}ï¼‰"
    if value <= 0:
        return None, "è‡ªåŠ¨è¯†åˆ«"
    return value, str(value)


def _get_db_url(config: Dict[str, str]) -> URL:
    return URL.create(
        drivername="mysql+pymysql",
        username=config["user"],
        password=config["password"] or None,
        host=config["host"],
        port=int(config["port"]),
        database=config["database"],
    )


def _ensure_business_database_exists(config: Dict[str, str]) -> None:
    ensure_database_exists_for_config(config)


def _get_business_engine(config: Dict[str, str]):
    _ensure_business_database_exists(config)
    return create_engine(_get_db_url(config), future=True)


st.set_page_config(page_title="ä¸šåŠ¡æ•°æ®ä¸Šä¼ ", layout="wide")
st.title("ğŸ“¥ ä¸šåŠ¡æ•°æ®ä¸Šä¼ ")

try:
    config = get_business_db_config()
except ValueError as exc:
    st.error(str(exc))
    st.stop()
skip_sheets = _parse_list_env(
    "SKIP_SHEETS", ["Event workflow", "Database Structure", "Cover Page"]
)
excel_display_row, excel_display_label = _parse_display_row_env(
    "DISPLAY_NAME_ROW"
)
csv_display_row, csv_display_label = _parse_display_row_env(
    "CSV_DISPLAY_NAME_ROW"
)

st.warning("æ³¨æ„ï¼šå¦‚æœè¡¨å·²å­˜åœ¨ï¼Œå°†ä¼šå…¨é‡æ›¿æ¢æ•°æ®åº“ä¸­çš„æ•°æ®ã€‚")

st.markdown("### å¯¼å…¥è§„åˆ™")
st.markdown(
    "\n".join(
        [
            "- å•æ–‡ä»¶ä¸Šä¼ ï¼ŒExcel å¯åŒ…å«å¤šä¸ª Sheetã€‚",
            "- è¡¨åè§„åˆ™ï¼šExcel ä½¿ç”¨ Sheet åç§°ï¼›CSV ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ã€‚",
            "- ç¦æ­¢ Sheet ååŒ…å« 'sheet'ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ï¼Œéœ€æ”¹ååä¸Šä¼ ã€‚",
            f"- è·³è¿‡ Sheetï¼š{', '.join(skip_sheets) if skip_sheets else 'æ— '}",
            f"- Excel æ˜¾ç¤ºåè¡Œé…ç½®ï¼š{excel_display_label}",
            f"- CSV æ˜¾ç¤ºåè¡Œé…ç½®ï¼š{csv_display_label}",
            "- è‡ªåŠ¨è¯†åˆ«æ˜¾ç¤ºåè¡Œæ—¶ï¼Œé»˜è®¤å¯¹æ¯”ç¬¬2è¡Œä¸ç¬¬3è¡Œçš„æ–‡æœ¬å æ¯”ã€‚",
            "- æ˜¾ç¤ºåå†™å…¥è§„åˆ™ï¼šä»…å½“ç³»ç»Ÿåº“ display_name ä¸ºç©ºæ—¶å†™å…¥ã€‚",
            "- å¯¼å…¥å‰åä¼šè‡ªåŠ¨åŒæ­¥å…ƒæ•°æ®ã€‚",
            "- åˆ—åå¼ºæ ¡éªŒï¼šä¸å…è®¸é‡å¤åˆ—åã€ç©ºåˆ—åæˆ– Unnamed åˆ—ã€‚",
        ]
    )
)

st.markdown("### å½“å‰ä¸šåŠ¡åº“")
st.markdown(
    "\n".join(
        [
            f"- CURRENT_BUSINESS_CODE: `{config['code']}`",
            f"- æ•°æ®åº“å: `{config['database']}`",
            f"- è¿æ¥åœ°å€: `{config['host']}:{config['port']}`",
        ]
    )
)

uploaded_file = st.file_uploader(
    "ä¸Šä¼  Excel æˆ– CSVï¼ˆå•æ–‡ä»¶ï¼‰",
    type=["xlsx", "xls", "csv"],
    accept_multiple_files=False,
)

if uploaded_file is not None:
    file_ext = Path(uploaded_file.name).suffix.lower()

    if st.button("å¼€å§‹å¯¼å…¥", type="primary"):
        try:
            init_system_db()
            logs: List[str] = []
            _ensure_business_database_exists(config)
            try:
                with st.spinner("å¯¼å…¥å‰åŒæ­¥å…ƒæ•°æ®..."):
                    pre_sync = sync_business_metadata()
                logs.append(
                    "å¯¼å…¥å‰åŒæ­¥ï¼š"
                    f"è¡¨/è§†å›¾ {pre_sync['objects_scanned']}ï¼Œ"
                    f"åˆ— {pre_sync['columns_scanned']}"
                )
            except Exception as exc:
                logs.append(f"å¯¼å…¥å‰åŒæ­¥å¤±è´¥: {exc}")
            engine = _get_business_engine(config)
            with st.spinner("æ­£åœ¨å¯¼å…¥..."):
                if file_ext in {".xlsx", ".xls"}:
                    import_logs, display_maps = upload_excel(
                        uploaded_file,
                        engine,
                        skip_sheets,
                        excel_display_row,
                    )
                elif file_ext == ".csv":
                    import_logs, display_maps = upload_csv(
                        uploaded_file,
                        engine,
                        csv_display_row,
                    )
                else:
                    raise FileFormatError("æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
            logs.extend(import_logs)
            st.success("å¯¼å…¥å®Œæˆã€‚")
            try:
                with st.spinner("å¯¼å…¥ååŒæ­¥å…ƒæ•°æ®..."):
                    post_sync = sync_business_metadata()
                logs.append(
                    "å¯¼å…¥ååŒæ­¥ï¼š"
                    f"è¡¨/è§†å›¾ {post_sync['objects_scanned']}ï¼Œ"
                    f"åˆ— {post_sync['columns_scanned']}"
                )
            except Exception as exc:
                logs.append(f"å¯¼å…¥ååŒæ­¥å¤±è´¥: {exc}")

            if display_maps:
                total_updates = 0
                for table_name, display_map in display_maps.items():
                    updated = update_business_column_display_names(
                        config["code"],
                        table_name,
                        display_map,
                        override=False,
                    )
                    total_updates += updated
                    logs.append(
                        f"æ˜¾ç¤ºåå†™å…¥ï¼š{table_name} "
                        f"æ›´æ–° {updated}/{len(display_map)}"
                    )
                logs.append(f"æ˜¾ç¤ºåå†™å…¥å®Œæˆï¼Œå…±æ›´æ–° {total_updates} ä¸ªã€‚")
            else:
                logs.append("æ˜¾ç¤ºåå†™å…¥ï¼šæ— å¯ç”¨æ˜ å°„ã€‚")

            if logs:
                st.code("\n".join(logs))
        except FileFormatError:
            st.error("æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
        except Exception as exc:
            st.error(f"å¯¼å…¥å¤±è´¥: {exc}")
